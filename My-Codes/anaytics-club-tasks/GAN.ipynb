{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTntg_D5mguC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==1.14\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNzoOnwWo4sP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xNq4xxJo58K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_img(x):\n",
        "    \"\"\"\n",
        "    The function to return the preprocessed images. \n",
        "    You dont need to preprocess for sure, but use this if you are!!\n",
        "    \n",
        "    Suggest some different ways to preprocess images!!!\n",
        "    \n",
        "    YOUR CODE GOES HERE\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalizing the image for easier weight calculations\n",
        "\n",
        "    x = x.astype('float32')\n",
        "    x /= 255\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r88igcBHo7PT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def import_mnist(preprocess=True):\n",
        "    \n",
        "    print(\"Downloading MNIST data ... \", end = \"\")\n",
        "\n",
        "    # downloads the data from the mnist dataset incase it is unavailble in your system - one time operation only.\n",
        "    from keras.datasets import mnist\n",
        "    \n",
        "    # loading data from the MNIST DIGIT dataset. We will be using only x_train as the model just needs to generate images \n",
        "    # which are real and indistinguishable. Hence we are not worried about which digit the model is outputing.\n",
        "    \n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    # reshaping the input to pass to keras - input shape = (number of images, image_dimensions, 1 - number of channels) : Black and white image hence single channel.\n",
        "    x_train = x_train.reshape(x_train.shape[0], 28,28,1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 28,28,1)\n",
        "    \n",
        "    \"\"\"\n",
        "    Preprocessing the images read from the dataset.\n",
        "    if preprocess:\n",
        "        x_train = preprocess_img(x_train)\n",
        "        x_test = preprocess_img(x_test)\n",
        "    \n",
        "    You dont need to do this for sure but still think about it!\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    YOUR CODE GOES HERE!\n",
        "    \"\"\"\n",
        "    if(preprocess):\n",
        "      x_train = preprocess_img(x_train)\n",
        "      x_test = preprocess_img(x_test)\n",
        "\n",
        "    print(\"Done\")\n",
        "    return x_train, y_train, x_test, y_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCnE95Slo8xZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Return a numpy array containing size number of randomly generated numbers each of length 100. HINT : use numpy random functions.\n",
        "\"\"\"\n",
        "def gen_noise(size):\n",
        "    \"\"\"\n",
        "    YOUR CODE GOES HERE!!\n",
        "    \"\"\"\n",
        "\n",
        "    return np.random.uniform(-1.0, 1.0, size=[size, 100])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CDQAGI9o_De",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Like i said everytime the entire GAN is trained the discriminator's weights are fixed. Hence this function is used to do that\n",
        "Given a network net it converts all its weights either to Frozen (fixed) - if False and back to trainable mode if True\n",
        "\"\"\"\n",
        "def make_trainable(net, val):\n",
        "    net.trainable = val\n",
        "    for l in net.layers:\n",
        "        l.trainable = val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymdn2GY8pAu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Plotting helper function - I did it for you, so take LITE!!!\n",
        "\"\"\"\n",
        "        \n",
        "def plot_large(img):\n",
        "    fig1 = plt.figure(figsize=(4,4))\n",
        "    ax1 = fig1.add_subplot(1,1,1)\n",
        "    ax1.axes.get_xaxis().set_visible(False)\n",
        "    ax1.axes.get_yaxis().set_visible(False)\n",
        "    ax1.imshow(img, cmap=\"gray\")\n",
        "    plt.show()\n",
        "def plot(generator,n=5):\n",
        "    img = np.zeros((n*28,1))\n",
        "    for i in range(n):\n",
        "        k = generator.predict(gen_noise(n)).reshape(n*28,28)\n",
        "        img = np.concatenate((img,k), axis=1)\n",
        "    plot_large(img[:,1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNJMEKR1pC9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DCGAN():\n",
        "\n",
        "    # the init function which is called automatically when we initialize an object of the class\n",
        "    def __init__(self, learning_rate, batch_size, num_epochs, save_path):\n",
        "        # the input, output specific parameters!!\n",
        "        self.img_shape = (28,28,1)\n",
        "        self.noise_dim = 100\n",
        "\n",
        "        # model specific parameters!!\n",
        "        self.lr = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.save_path = save_path # where we save the models weights in a regular basis.\n",
        "\n",
        "        # defining the generator and discriminator.\n",
        "        \"\"\"\n",
        "        Use the helper functions in this class to define the generator and discriminator!!!\n",
        "        Use a sequential model to define the gan_model using keras.Sequential to connect the generator to the discriminator.\n",
        "\n",
        "        YOUR CODE GOES HERE .......\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.discriminator = self.create_disc()\n",
        "        self.generator = self.create_gen()\n",
        "\n",
        "        self.gan_model = keras.Sequential()\n",
        "        self.gan_model.add(self.generator)\n",
        "        self.gan_model.add(self.discriminator)\n",
        "\n",
        "        print(\"GAN Model : \")\n",
        "        print(self.gan_model.summary())\n",
        "        # compile the models - I have done this for you .....................\n",
        "        \"\"\"\n",
        "        BONUS : what is binary cross entropy and why is it used here. I am asking very simple stuff :P (easy bonus)\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        BONUS ANSWER :\n",
        "        \n",
        "        Binary Cross-Entropy is the loss function usually associated with binary output labels.\n",
        "        \n",
        "        It is used here, since the output is binary (Real/Fake)\n",
        "        \"\"\"\n",
        "        self.discriminator.compile(keras.optimizers.Adam(2*self.lr), \"binary_crossentropy\")\n",
        "        self.gan_model.compile(keras.optimizers.Adam(self.lr), \"binary_crossentropy\")\n",
        "\n",
        "    \"\"\"\n",
        "    create_gen function to define the generator model.\n",
        "    Use the keras Sequential models to define the generator. Visit keras documentation page for more info!!\n",
        "\n",
        "    For example:\n",
        "    model = keras.Sequential()\n",
        "    layer1 = keras.layers.Dense(n, activation=\"relu\")\n",
        "    model.add(layer1)\n",
        "    return model\n",
        "    \"\"\"\n",
        "    def create_gen(self):\n",
        "        \"\"\"\n",
        "        model = keras.Sequential()\n",
        "        layer1 = ...\n",
        "        layer2 = .....\n",
        "        layer3 = .....\n",
        "        ..\n",
        "\n",
        "\n",
        "        model.add(layer1)\n",
        "        model.add(layer2)\n",
        "        ....\n",
        "\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "        YOUR CODE GOES HERE!!!!\n",
        "        \"\"\"\n",
        "\n",
        "        model = keras.Sequential()\n",
        "\n",
        "        model.add(keras.layers.Dense(256*7*7, input_shape = (self.noise_dim,)))\n",
        "        model.add(keras.layers.BatchNormalization())\n",
        "        model.add(keras.layers.LeakyReLU(0.2))\n",
        "        model.add(keras.layers.Reshape((7,7,256)))\n",
        "        model.add(keras.layers.Dropout(0.4))\n",
        "\n",
        "        model.add(keras.layers.UpSampling2D())\n",
        "        model.add(keras.layers.Conv2DTranspose(128, kernel_size = 5, strides = 1, padding='same'))\n",
        "        model.add(keras.layers.BatchNormalization(momentum=0.9))\n",
        "        model.add(keras.layers.LeakyReLU(alpha=0.01))\n",
        "\n",
        "        model.add(keras.layers.UpSampling2D())\n",
        "        model.add(keras.layers.Conv2DTranspose(64, kernel_size=5, strides=1, padding='same'))\n",
        "        model.add(keras.layers.BatchNormalization(momentum=0.9))\n",
        "        model.add(keras.layers.LeakyReLU(alpha=0.2))\n",
        "\n",
        "        model.add(keras.layers.Conv2DTranspose(32, kernel_size=5, strides=1, padding='same'))\n",
        "        model.add(keras.layers.BatchNormalization(momentum=0.9))\n",
        "        model.add(keras.layers.LeakyReLU(alpha=0.2))\n",
        "\n",
        "        model.add(keras.layers.Conv2DTranspose(1, kernel_size = 5, strides = 1, padding='same'))\n",
        "        model.add(keras.layers.Activation('sigmoid'))\n",
        "\n",
        "        print(\"Generator model = \")\n",
        "        print(model.summary())\n",
        "        return model\n",
        "\n",
        "    \"\"\"\n",
        "    create_disc function is used to create the discriminator for the model.\n",
        "    Use the same method as above to define the model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def create_disc(self):\n",
        "        \"\"\"\n",
        "        model = keras.Sequential()\n",
        "        layer1 = ...\n",
        "        layer2 = .....\n",
        "        layer3 = .....\n",
        "        ..\n",
        "\n",
        "\n",
        "        model.add(layer1)\n",
        "        model.add(layer2)\n",
        "        ....\n",
        "\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "        YOUR CODE GOES HERE!!!!\n",
        "        \"\"\"\n",
        "\n",
        "        model = keras.Sequential()\n",
        "        model.add(keras.layers.Conv2D(64,kernel_size=5,strides=2,padding='same',input_shape=self.img_shape))\n",
        "        model.add(keras.layers.BatchNormalization(momentum=0.9))\n",
        "        model.add(keras.layers.ReLU())\n",
        "        model.add(keras.layers.Dropout(0.4))\n",
        "        model.add(keras.layers.Conv2D(128,kernel_size=5,strides=2,padding='same'))\n",
        "        model.add(keras.layers.BatchNormalization(momentum=0.9))\n",
        "        model.add(keras.layers.ReLU())\n",
        "        model.add(keras.layers.Dropout(0.4))\n",
        "        model.add(keras.layers.Conv2D(256,kernel_size=5,strides=2,padding='same'))\n",
        "        model.add(keras.layers.BatchNormalization(momentum=0.9))\n",
        "        model.add(keras.layers.ReLU())\n",
        "        model.add(keras.layers.Dropout(0.4))\n",
        "        model.add(keras.layers.Conv2D(512,kernel_size=5,strides=1,padding='same'))\n",
        "        model.add(keras.layers.BatchNormalization(momentum=0.9))\n",
        "        model.add(keras.layers.ReLU())\n",
        "        model.add(keras.layers.Dropout(0.4))\n",
        "        model.add(keras.layers.Flatten())\n",
        "        model.add(keras.layers.Dense(1))\n",
        "        model.add(keras.layers.Activation('sigmoid'))\n",
        "\n",
        "        print(\"Discriminator model = \")\n",
        "        print(model.summary())\n",
        "        return model\n",
        "\n",
        "    # function to pretrain the discriminator model to identify real and fake images!!!\n",
        "    def pretrain(self, x):\n",
        "        # how many examples to choose\n",
        "        size = x.shape[0]//200\n",
        "\n",
        "        \"\"\"\n",
        "        randomly choose size number of images from x - real images\n",
        "        Similarly create size number of fake images - how will you do this ! (brain time :P)\n",
        "\n",
        "        Now decide on a convention - Lets say - 0 - Fake images, 1 - Real images\n",
        "\n",
        "        For example:\n",
        "        real_img = ...\n",
        "        fake_img = ...\n",
        "        x = np.concatenate(real_img, fake_img)\n",
        "        labels = [1]*size + [0]*size - x contains first size number of real images (label 1) and next size number of fake images (label 0)\n",
        "        self.discriminator.fit(x, labels, batch_size = self.batch_size, epochs = 1)\n",
        "\n",
        "\n",
        "\n",
        "        YOUR CODE GOES HERE!!\n",
        "        \"\"\"\n",
        "\n",
        "        x,y = self.gen_data(x,size)\n",
        "\n",
        "        self.discriminator.fit(x,y,batch_size = self.batch_size,epochs = 1)\n",
        "\n",
        "        print(\"\\n\\nPretraining done!\\n\\n\")\n",
        "\n",
        "\n",
        "    # function to generate data for the discriminator to train on in every iteration!\n",
        "    def gen_data(self, x, size):\n",
        "        \"\"\"\n",
        "        randomly choose size number of real images from x - real images\n",
        "        also create size number of fake images - think think !!!!!\n",
        "        concatenate them and create labels for the same and hence return them\n",
        "\n",
        "\n",
        "        return x, labels\n",
        "\n",
        "        YOUR CODE GOES HERE!!!\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        if(size>len(x)):\n",
        "          raise ValueError(\"Size requested greater than size of x\")\n",
        "        \n",
        "        real = x[np.random.choice(range(6000),size,replace=False)]\n",
        "        fake = self.generator.predict(gen_noise(size))\n",
        "        images = np.concatenate((real,fake))\n",
        "        labels = np.concatenate((np.ones(size),np.zeros(size)))\n",
        "        return np.array(images),np.array(labels)\n",
        "\n",
        "\n",
        "    # the training function. Loop through every epoch and perform multiple iterations in each epoch with each iteration for a batch-size number of images!!\n",
        "    def train(self, x, num_iter):\n",
        "        # iterating through num_epochs!!\n",
        "\n",
        "        for i in range(self.num_epochs):\n",
        "            print(\"\\n\\nEpoch no :\"+str(i+1)+\"/\"+str(self.num_epochs)+\"\\n\\n\")\n",
        "\n",
        "\n",
        "            # iterating through num of iterations!!!\n",
        "            for j in tqdm(range(num_iter)):\n",
        "                # use the gen_data function to create input and output for the discriminator!!\n",
        "                x1,y = self.gen_data(x, self.batch_size//2)\n",
        "\n",
        "                # train the discriminator on this data!!!\n",
        "                loss = self.discriminator.train_on_batch(x1,y)\n",
        "                \n",
        "                print(\"\\nLoss in discriminator : \"+str(loss),end=\"\\n\\n\")\n",
        "\n",
        "                # Freeze the discriminator to train the GAN model - fix the weights!!\n",
        "                # make_trainable(self.discriminator, False)\n",
        "\n",
        "                # train the gan model!!!!!!!!\n",
        "\n",
        "                loss = self.gan_model.train_on_batch(np.random.uniform(-1.0, 1.0, size=[self.batch_size, 100]),np.ones([self.batch_size, 1]))\n",
        "\n",
        "\n",
        "                print(\"\\nLoss in gan : \"+str(loss),end=\"\\n\\n\")\n",
        "\n",
        "                # make the discriminator params back to trainable for the next iteration!!!\n",
        "                # make_trainable(self.discriminator, True)\n",
        "\n",
        "            #save the weights and plot the results every 10 epochs!!!\n",
        "            if i%10 == 0:\n",
        "              self.gan_model.save_weights(self.save_path + str(i+1)+\".h5\")\n",
        "            plot(self.generator)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEdhgU10pIP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(lr, batch_size, num_epochs, save_path):\n",
        "    \n",
        "    # Loading the dataset using the function import_mnist present in utils.py\n",
        "    x,_,_,_ = import_mnist(preprocess=True)\n",
        "    \n",
        "    # defining an object of the Class DCGAN which is going to be our gan model. This function calls the init function of the class DCGAN\n",
        "    gan_model = DCGAN(lr, batch_size, num_epochs, save_path)\n",
        "    \n",
        "    \"\"\"\n",
        "    YOUR CODE GOES HERE!!!......................... (use functions defined in the DCGAN CLASS)\n",
        "    \n",
        "    Start training the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # gan_model.pretrain(x)\n",
        "\n",
        "    gan_model.train(x,1)\n",
        "    \n",
        "    print(\"Training Done!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeROGQpZpJti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 1e-4\n",
        "batch_size = 128\n",
        "num_epochs = 100\n",
        "save_path = \"./weights/\"\n",
        "\n",
        "main(lr, batch_size, num_epochs, save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}